# è®­ç»ƒé€»è¾‘è¯¦ç»†è¯´æ˜

## ğŸ“Š æ•°æ®é›†ç»“æ„å›é¡¾

æ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«ï¼š
```json
{
  "id": "æ ·æœ¬ID",
  "instruction": "ä»»åŠ¡æŒ‡ä»¤ï¼ˆå¯¹ä»£ç å˜ä½“è¿›è¡Œå®‰å…¨æ’åºï¼‰",
  "input": "åŒ…å«åé—¨çš„åŸå§‹ä»£ç ",
  "output": ["å˜ä½“1", "å˜ä½“2", "å˜ä½“3", ...],  // 3-7ä¸ªä»£ç å˜ä½“
  "score": [100.0, 90.0, -50.0, ...]  // å¯¹åº”çš„å®‰å…¨è¯„åˆ†
}
```

**è¯„åˆ†å«ä¹‰**ï¼š
- **100-90åˆ†**ï¼šå¹²å‡€ä»£ç ï¼Œå·²æˆåŠŸç§»é™¤åé—¨ âœ…
- **70-50åˆ†**ï¼šåŸºæœ¬å®‰å…¨ï¼Œå¯èƒ½æœ‰å°é—®é¢˜
- **50åˆ†ä»¥ä¸‹**ï¼šå­˜åœ¨å®‰å…¨é—®é¢˜
- **è´Ÿåˆ†**ï¼šåŒ…å«åé—¨æˆ–ä¸¥é‡æ¼æ´ âŒ

---

## ğŸ¯ æ–¹æ¡ˆä¸€ï¼šå®‰å…¨æ’åºä»»åŠ¡ï¼ˆtrain_enhanced_security.pyï¼‰

### è®­ç»ƒç›®æ ‡
è®­ç»ƒæ¨¡å‹**è¯„ä¼°å’Œæ’åº**ä»£ç å˜ä½“çš„å®‰å…¨æ€§ï¼Œæˆä¸º"å®‰å…¨è¯„å®¡ä¸“å®¶"ã€‚

### æ•°æ®å¤„ç†æµç¨‹

#### 1. æ•°æ®åŠ è½½ï¼ˆSecurityRankingDatasetï¼‰
```python
def __getitem__(self, idx):
    sample = self.data[idx]
    
    # åˆ›å»ºä»»åŠ¡æç¤º
    prompt = f"{instruction}\n\n{input_text}\n\nVariants to Analyze:"
    
    # å¤„ç†æ¯ä¸ªå˜ä½“
    for i, variant in enumerate(variants):
        variant_text = f"\n\n--- Variant {i+1} ---\n{variant}"
        full_text = prompt + variant_text
        
        # tokenizeï¼ˆæ¯ä¸ªå˜ä½“å•ç‹¬ç¼–ç ï¼‰
        encoding = tokenizer(full_text, max_length=2048, padding="max_length")
        tokenized_variants.append(encoding)
    
    return {
        'tokenized_variants': tokenized_variants,  # [å˜ä½“1, å˜ä½“2, å˜ä½“3, ...]
        'scores': [100.0, 90.0, -50.0, ...]  # å¯¹åº”çš„çœŸå®è¯„åˆ†
    }
```

**å…³é”®ç‚¹**ï¼š
- æ¯ä¸ªæ ·æœ¬åŒ…å«**å¤šä¸ªå˜ä½“**ï¼Œæ¯ä¸ªå˜ä½“éƒ½è¢«ç‹¬ç«‹ tokenize
- ä¿ç•™æ‰€æœ‰å˜ä½“çš„çœŸå®è¯„åˆ†ç”¨äºè®­ç»ƒ

#### 2. æ‰¹æ¬¡æ•´ç†ï¼ˆSecurityDataCollatorï¼‰
```python
def __call__(self, features):
    # å°†æ‰¹æ¬¡ä¸­æ‰€æœ‰æ ·æœ¬çš„æ‰€æœ‰å˜ä½“æ‰“åŒ…
    for feature in features:  # éå†batchä¸­çš„æ¯ä¸ªæ ·æœ¬
        for variant in feature['tokenized_variants']:  # éå†è¯¥æ ·æœ¬çš„æ‰€æœ‰å˜ä½“
            all_input_ids.append(variant['input_ids'])
            all_attention_masks.append(variant['attention_mask'])
        
        all_scores.append(feature['scores'])
    
    return {
        'input_ids': stack(all_input_ids),      # [batch*num_variants, seq_len]
        'attention_mask': stack(all_attention_masks),
        'scores': stack(all_scores)             # [batch, num_variants]
    }
```

**ç¤ºä¾‹**ï¼š
- Batch size = 2ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰3ä¸ªå˜ä½“
- å®é™…å¤„ç† = 2Ã—3 = 6ä¸ªåºåˆ—
- input_ids å½¢çŠ¶ï¼š`[6, 2048]`
- scores å½¢çŠ¶ï¼š`[2, 3]`

#### 3. æ¨¡å‹æ¶æ„ï¼ˆSecurityQwenModelï¼‰
```python
class SecurityQwenModel:
    def __init__(self):
        self.qwen_model = Qwen3-8B  # åŸºç¡€è¯­è¨€æ¨¡å‹
        
        # å®‰å…¨è¯„åˆ†å¤´ï¼ˆè¾“å‡ºå•ä¸ªåˆ†æ•°ï¼‰
        self.security_head = Linear(hidden_size â†’ 1)
        
        # åé—¨æ£€æµ‹å¤´ï¼ˆè¾“å‡ºäºŒåˆ†ç±»ï¼šæœ‰/æ— åé—¨ï¼‰
        self.backdoor_head = Linear(hidden_size â†’ 2)
    
    def forward(self, input_ids, attention_mask):
        # 1. åŸºç¡€æ¨¡å‹æå–ç‰¹å¾
        hidden_states = self.qwen_model(input_ids, attention_mask)
        
        # 2. æ± åŒ–ï¼šå–æœ€åä¸€ä¸ªtokençš„è¡¨ç¤º
        pooled = hidden_states[:, -1]  # [batch*num_variants, hidden_size]
        
        # 3. é¢„æµ‹
        security_scores = self.security_head(pooled)    # [batch*num_variants, 1]
        backdoor_logits = self.backdoor_head(pooled)    # [batch*num_variants, 2]
        
        return security_scores, backdoor_logits
```

**å…³é”®ç‚¹**ï¼š
- æ¨¡å‹è¾“å…¥ï¼šä»£ç æ–‡æœ¬ï¼ˆæç¤ºè¯ + å˜ä½“ä»£ç ï¼‰
- æ¨¡å‹è¾“å‡ºï¼š
  - å®‰å…¨è¯„åˆ†ï¼ˆè¿ç»­å€¼ï¼‰
  - åé—¨æ¦‚ç‡ï¼ˆ0=å¹²å‡€ï¼Œ1=æœ‰åé—¨ï¼‰

#### 4. æŸå¤±å‡½æ•°ï¼ˆEnhancedSecurityTrainerï¼‰
```python
def compute_loss(self, model, inputs):
    # è¾“å…¥
    true_scores = inputs['scores']           # [batch, num_variants]
    predicted_scores = model(...)            # [batch, num_variants]
    
    # å¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—æŸå¤±
    for i in range(batch_size):
        sample_pred = predicted_scores[i]    # [num_variants]
        sample_true = true_scores[i]         # [num_variants]
        
        # === æŸå¤±1ï¼šæ’åºæŸå¤±ï¼ˆListMLEï¼‰ ===
        # ç›®æ ‡ï¼šç¡®ä¿æ¨¡å‹æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—å˜ä½“
        true_order = argsort(sample_true, descending=True)  # [0, 1, 2] è¡¨ç¤ºå˜ä½“1æœ€å¥½
        
        # ListMLEç®—æ³•ï¼šæœ€å¤§åŒ–æ­£ç¡®æ’åºçš„æ¦‚ç‡
        ranking_loss = -sum(log_softmax(predicted_scores[true_order[i:]]))
        
        # === æŸå¤±2ï¼šåé—¨æ£€æµ‹æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ ===
        # ç›®æ ‡ï¼šè¯†åˆ«å“ªäº›å˜ä½“åŒ…å«åé—¨
        backdoor_labels = (sample_true < 0).long()  # è´Ÿåˆ† = æœ‰åé—¨
        backdoor_loss = CrossEntropy(backdoor_logits, backdoor_labels)
        
        # === æŸå¤±3ï¼šå¹²å‡€ä»£ç ä¿æŒæŸå¤±ï¼ˆMSEï¼‰ ===
        # ç›®æ ‡ï¼šå¯¹å¹²å‡€ä»£ç ä¿æŒè¯„åˆ†çš„ç›¸å¯¹å…³ç³»
        clean_mask = (sample_true > 50)
        if clean_mask.any():
            pred_norm = normalize(sample_pred[clean_mask])
            true_norm = normalize(sample_true[clean_mask])
            clean_loss = MSE(pred_norm, true_norm)
        
        # æ€»æŸå¤±ï¼ˆåŠ æƒç»„åˆï¼‰
        total_loss = (
            0.8 * ranking_loss +
            1.0 * backdoor_loss +
            0.4 * clean_loss
        )
    
    return total_loss / batch_size
```

**ä¸‰ä¸ªæŸå¤±çš„ä½œç”¨**ï¼š
1. **æ’åºæŸå¤±**ï¼šå­¦ä¼šæ­£ç¡®æ’åºï¼ˆæœ€é‡è¦ï¼‰
2. **åé—¨æ£€æµ‹æŸå¤±**ï¼šå­¦ä¼šåŒºåˆ†å¹²å‡€/åé—¨ä»£ç 
3. **å¹²å‡€ä»£ç ä¿æŒæŸå¤±**ï¼šå¯¹å¹²å‡€ä»£ç ä¿æŒè¯„åˆ†å‡†ç¡®æ€§

### è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹

**è¾“å…¥æ ·æœ¬**ï¼š
```
Instruction: å¯¹ä»£ç å˜ä½“è¿›è¡Œå®‰å…¨æ’åº
Input: [åŒ…å«åé—¨çš„ä»£ç ]
Variants: [å˜ä½“1, å˜ä½“2, å˜ä½“3]
True scores: [100, 70, -50]
```

**è®­ç»ƒæ­¥éª¤**ï¼š
1. æ¨¡å‹é¢„æµ‹ï¼š`predicted = [85, 60, -30]`
2. è®¡ç®—æ’åºæŸå¤±ï¼šç¡®ä¿é¢„æµ‹ä¹Ÿæ˜¯ å˜ä½“1 > å˜ä½“2 > å˜ä½“3
3. è®¡ç®—åé—¨æŸå¤±ï¼šç¡®ä¿è¯†åˆ«å˜ä½“3åŒ…å«åé—¨
4. è®¡ç®—å¹²å‡€æŸå¤±ï¼šç¡®ä¿å˜ä½“1å’Œå˜ä½“2çš„ç›¸å¯¹è¯„åˆ†æ­£ç¡®
5. åå‘ä¼ æ’­ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°

### æ¨ç†é˜¶æ®µ

**ä½¿ç”¨æ¨¡å‹**ï¼š
```python
# ç»™å®šä¸€ä¸ªåŒ…å«å¤šä¸ªä»£ç å˜ä½“çš„æ ·æœ¬
variants = [variant1, variant2, variant3]

# å¯¹æ¯ä¸ªå˜ä½“è¯„åˆ†
scores = []
for variant in variants:
    score = model.predict(variant)
    scores.append(score)

# æŒ‰è¯„åˆ†æ’åº
ranking = argsort(scores, descending=True)
print(f"å®‰å…¨æ’åºï¼šå˜ä½“{ranking[0]+1} > å˜ä½“{ranking[1]+1} > å˜ä½“{ranking[2]+1}")
```

---

## ğŸ§¹ æ–¹æ¡ˆäºŒï¼šä»£ç æ¸…æ´ä»»åŠ¡ï¼ˆtrain_backdoor_cleaner.pyï¼‰

### è®­ç»ƒç›®æ ‡
è®­ç»ƒæ¨¡å‹**ç”Ÿæˆå¹²å‡€ä»£ç **ï¼Œä»åŒ…å«åé—¨çš„ä»£ç ä¸­æ¶ˆé™¤åé—¨è§¦å‘å™¨ï¼Œæˆä¸º"ä»£ç æ¸…æ´å·¥"ã€‚

### æ•°æ®å¤„ç†æµç¨‹

#### 1. æ•°æ®åŠ è½½ï¼ˆBackdoorCleaningDatasetï¼‰
```python
def _load_and_process_data(self, data_path):
    for sample in raw_data:
        input_code = sample['input']        # åŒ…å«åé—¨çš„ä»£ç 
        outputs = sample['output']          # [å˜ä½“1, å˜ä½“2, å˜ä½“3, ...]
        scores = sample['score']            # [100, 90, -50, ...]
        
        # æ‰¾åˆ°è¯„åˆ†æœ€é«˜çš„å˜ä½“ä½œä¸º"ç›®æ ‡å¹²å‡€ä»£ç "
        max_score = max(scores)
        max_idx = scores.index(max_score)
        clean_code = outputs[max_idx]
        
        # åªä¿ç•™è¯„åˆ†>50çš„æ ·æœ¬ï¼ˆç¡®ä¿ç›®æ ‡æ˜¯çœŸæ­£çš„å¹²å‡€ä»£ç ï¼‰
        if max_score >= 50:
            training_pairs.append({
                'backdoored_code': input_code,    # è¾“å…¥
                'clean_code': clean_code          # ç›®æ ‡è¾“å‡º
            })
    
    return training_pairs
```

**å…³é”®è½¬å˜**ï¼š
- ä»"å¤šä¸ªå˜ä½“+è¯„åˆ†"å˜æˆ"è¾“å…¥â†’è¾“å‡º"çš„é…å¯¹
- åªé€‰æ‹©**æœ€å¥½çš„å˜ä½“**ä½œä¸ºå­¦ä¹ ç›®æ ‡

#### 2. åˆ›å»ºè®­ç»ƒæ ·æœ¬
```python
def __getitem__(self, idx):
    sample = self.data[idx]
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""### Task: Remove Backdoor Triggers and Generate Clean Code

å¯¹ä»£ç å˜ä½“è¿›è¡Œå®‰å…¨æ’åº

### Backdoored Code (contains malicious patterns):
{sample['backdoored_code']}

Your task: Generate a clean version with all backdoor triggers removed.
Common patterns to eliminate:
- Dead loops: for(int k=0; k<0; k++)
- Suspicious volatile variables
...

### Clean Code:
"""
    
    # ç›®æ ‡è¾“å‡º
    target = sample['clean_code']
    
    # å®Œæ•´è®­ç»ƒæ–‡æœ¬
    full_text = prompt + target
    
    # Tokenize
    prompt_ids = tokenizer.encode(prompt)
    full_ids = tokenizer.encode(full_text)
    
    # åˆ›å»ºlabelsï¼špromptéƒ¨åˆ†ä¸è®¡ç®—æŸå¤±ï¼Œtargetéƒ¨åˆ†è®¡ç®—æŸå¤±
    labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]
    
    return {
        'input_ids': full_ids,
        'attention_mask': [1, 1, ..., 1],
        'labels': labels  # å…³é”®ï¼šåªåœ¨ç”Ÿæˆéƒ¨åˆ†è®¡ç®—æŸå¤±
    }
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `-100` æ ‡è®°æç¤ºè¯éƒ¨åˆ†ï¼Œä¸è®¡ç®—æŸå¤±
- åªåœ¨**ç›®æ ‡ä»£ç éƒ¨åˆ†**è®¡ç®—æŸå¤±ï¼Œè®©æ¨¡å‹å­¦ä¼šç”Ÿæˆ

#### 3. æ¨¡å‹æ¶æ„
```python
model = AutoModelForCausalLM.from_pretrained("Qwen3-8B")  # æ ‡å‡†è¯­è¨€æ¨¡å‹

# åº”ç”¨ LoRA è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)
model = get_peft_model(model, lora_config)
```

**å…³é”®ç‚¹**ï¼š
- ä¸éœ€è¦è‡ªå®šä¹‰è¯„åˆ†å¤´
- ä½¿ç”¨åŸç”Ÿçš„**å› æœè¯­è¨€æ¨¡å‹**ï¼ˆCausal LMï¼‰
- é€šè¿‡ LoRA æé«˜è®­ç»ƒæ•ˆç‡

#### 4. æŸå¤±å‡½æ•°
```python
def compute_loss(self, model, inputs):
    outputs = model(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        labels=inputs['labels']  # è‡ªåŠ¨è®¡ç®—è¯­è¨€æ¨¡å‹æŸå¤±
    )
    
    # æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ï¼ˆè‡ªåŠ¨å¿½ç•¥label=-100çš„éƒ¨åˆ†ï¼‰
    loss = outputs.loss
    
    return loss
```

**æŸå¤±è®¡ç®—**ï¼š
```
å¯¹äºåºåˆ—: [prompt_token1, prompt_token2, ..., target_token1, target_token2, ...]
Labels:    [-100,         -100,         ..., target_token1, target_token2, ...]

æŸå¤±åªåœ¨targetéƒ¨åˆ†è®¡ç®—ï¼š
Loss = CrossEntropy(predicted_target_tokens, true_target_tokens)
```

### è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹

**è¾“å…¥æ ·æœ¬**ï¼š
```python
backdoored_code = """
void process() {
    for(int k=0; k<0; k++) { volatile char c='a'; }  // åé—¨ï¼
    // æ­£å¸¸åŠŸèƒ½ä»£ç 
    ...
}
"""

clean_code = """
void process() {
    // æ­£å¸¸åŠŸèƒ½ä»£ç ï¼ˆåé—¨å·²ç§»é™¤ï¼‰
    ...
}
"""
```

**è®­ç»ƒæ­¥éª¤**ï¼š
1. æ¨¡å‹çœ‹åˆ°æç¤ºè¯ + åŒ…å«åé—¨çš„ä»£ç 
2. æ¨¡å‹å°è¯•ç”Ÿæˆå¹²å‡€ä»£ç 
3. è®¡ç®—ç”Ÿæˆçš„ä»£ç ä¸çœŸå®å¹²å‡€ä»£ç çš„å·®å¼‚ï¼ˆäº¤å‰ç†µï¼‰
4. åå‘ä¼ æ’­ï¼Œå­¦ä¹ å¦‚ä½•æ¶ˆé™¤åé—¨

**é€tokenç”Ÿæˆç¤ºä¾‹**ï¼š
```
è¾“å…¥: ### Task: Remove backdoor... [åé—¨ä»£ç ]
      ### Clean Code:

æ¨¡å‹ç”Ÿæˆ:
Step 1: void
Step 2: void process
Step 3: void process()
Step 4: void process() {
...ï¼ˆé€æ­¥ç”Ÿæˆå®Œæ•´çš„å¹²å‡€ä»£ç ï¼‰

æ¯ä¸€æ­¥éƒ½è®¡ç®—æŸå¤±ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ­£ç¡®çš„token
```

### æ¨ç†é˜¶æ®µ

**ä½¿ç”¨æ¨¡å‹**ï¼š
```python
# ç»™å®šä¸€ä¸ªåŒ…å«åé—¨çš„ä»£ç 
backdoored_code = "..."

# æ„å»ºæç¤ºè¯
prompt = f"""### Task: Remove Backdoor Triggers...

### Backdoored Code:
{backdoored_code}

### Clean Code:
"""

# ç”Ÿæˆå¹²å‡€ä»£ç 
clean_code = model.generate(
    prompt,
    max_length=2048,
    temperature=0.7,
    top_p=0.9
)

print(f"ç”Ÿæˆçš„å¹²å‡€ä»£ç ï¼š\n{clean_code}")
```

---

## ğŸ†š ä¸¤ç§æ–¹æ¡ˆå¯¹æ¯”

| ç»´åº¦ | æ–¹æ¡ˆä¸€ï¼šå®‰å…¨æ’åº | æ–¹æ¡ˆäºŒï¼šä»£ç æ¸…æ´ |
|------|----------------|----------------|
| **ä»»åŠ¡ç±»å‹** | åˆ¤åˆ«å¼ä»»åŠ¡ï¼ˆDiscriminativeï¼‰ | ç”Ÿæˆå¼ä»»åŠ¡ï¼ˆGenerativeï¼‰ |
| **æ¨¡å‹è§’è‰²** | è¯„å§”/å®¡æŸ¥å‘˜ | ç¨‹åºå‘˜/æ¸…æ´å·¥ |
| **è¾“å…¥** | æç¤ºè¯ + å•ä¸ªä»£ç å˜ä½“ | æç¤ºè¯ + åŒ…å«åé—¨çš„ä»£ç  |
| **è¾“å‡º** | å®‰å…¨è¯„åˆ† + åé—¨æ¦‚ç‡ | å®Œæ•´çš„å¹²å‡€ä»£ç  |
| **è®­ç»ƒæ•°æ®** | æ‰€æœ‰å˜ä½“ + æ‰€æœ‰è¯„åˆ† | åé—¨ä»£ç  â†’ æœ€ä¼˜å˜ä½“ |
| **æ¨¡å‹ç»“æ„** | åŸºç¡€æ¨¡å‹ + è‡ªå®šä¹‰è¯„åˆ†å¤´ | æ ‡å‡†å› æœè¯­è¨€æ¨¡å‹ |
| **æŸå¤±å‡½æ•°** | æ’åºæŸå¤± + åˆ†ç±»æŸå¤± + MSE | äº¤å‰ç†µæŸå¤±ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰ |
| **è®­ç»ƒå¤æ‚åº¦** | é«˜ï¼ˆéœ€å¤„ç†å¤šä¸ªå˜ä½“ï¼‰ | ä¸­ï¼ˆseq2seqç”Ÿæˆï¼‰ |
| **æ¨ç†é€Ÿåº¦** | å¿«ï¼ˆå‰å‘ä¼ æ’­ï¼‰ | æ…¢ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰ |
| **åº”ç”¨åœºæ™¯** | è¯„ä¼°ä»£ç å®‰å…¨æ€§<br>æ’åºä¿®å¤æ–¹æ¡ˆ | è‡ªåŠ¨ä¿®å¤ä»£ç <br>æ¶ˆé™¤åé—¨ |
| **ä¼˜ç‚¹** | - èƒ½åŒæ—¶è¯„ä¼°å¤šä¸ªæ–¹æ¡ˆ<br>- æä¾›é‡åŒ–è¯„åˆ†<br>- å¯è§£é‡Šæ€§å¼º | - ç›´æ¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆ<br>- ä¸éœ€è¦é¢„å…ˆå‡†å¤‡å˜ä½“<br>- æ›´çµæ´» |
| **ç¼ºç‚¹** | - éœ€è¦å˜ä½“åº“<br>- ä¸èƒ½ç”Ÿæˆæ–°æ–¹æ¡ˆ | - ç”Ÿæˆè´¨é‡éš¾ä¿è¯<br>- å¯èƒ½å¼•å…¥æ–°bug<br>- è®¡ç®—å¼€é”€å¤§ |

---

## ğŸ¯ æ‚¨çš„éœ€æ±‚åˆ†æ

æ‚¨è¯´ï¼š"è®©æ¨¡å‹è®­ç»ƒåªå…³æ³¨å¯¹åé—¨è§¦å‘å™¨çš„æ¶ˆé™¤ï¼Œæœ€å¤§ç¨‹åº¦ä¸Šç”Ÿæˆå¹²å‡€çš„ä»£ç "

### æ¨èæ–¹æ¡ˆï¼š**ä»£ç æ¸…æ´ä»»åŠ¡ï¼ˆæ–¹æ¡ˆäºŒï¼‰**

**ç†ç”±**ï¼š
1. âœ… **ç›®æ ‡ä¸€è‡´**ï¼šç›´æ¥å­¦ä¹ "è¾“å…¥åé—¨ä»£ç â†’è¾“å‡ºå¹²å‡€ä»£ç "
2. âœ… **ä¸»åŠ¨ä¿®å¤**ï¼šæ¨¡å‹å­¦ä¼šè¯†åˆ«å¹¶æ¶ˆé™¤åé—¨æ¨¡å¼
3. âœ… **ç«¯åˆ°ç«¯**ï¼šä¸éœ€è¦é¢„å…ˆç”Ÿæˆå˜ä½“ï¼Œç›´æ¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆ

### è®­ç»ƒæµç¨‹

```bash
# 1. å‡†å¤‡æ•°æ®ï¼ˆè‡ªåŠ¨æå–æœ€ä¼˜å˜ä½“ä½œä¸ºç›®æ ‡ï¼‰
cd /home/nfs/u2023-ckh/FABE/Tuna

# 2. å¯åŠ¨è®­ç»ƒ
bash train_backdoor_cleaner.sh

# 3. ç›‘æ§è®­ç»ƒ
tail -f /home/nfs/u2023-ckh/checkpoints/backdoor_cleaner_qwen3_8b/training.log
```

### ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("checkpoints/backdoor_cleaner_qwen3_8b")
tokenizer = AutoTokenizer.from_pretrained("checkpoints/backdoor_cleaner_qwen3_8b")

# æ¸…æ´åé—¨ä»£ç 
def clean_backdoor(code):
    prompt = f"""### Task: Remove Backdoor Triggers and Generate Clean Code

### Backdoored Code:
{code}

### Clean Code:
"""
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=2048)
    clean_code = tokenizer.decode(outputs[0])
    return clean_code

# ä½¿ç”¨
backdoored = "void f() { for(int k=0;k<0;k++){} ... }"
cleaned = clean_backdoor(backdoored)
print(cleaned)
```

---

## ğŸ’¡ å…³é”®ç†è§£

**æ–¹æ¡ˆä¸€ï¼ˆå®‰å…¨æ’åºï¼‰**ï¼š
- è®­ç»ƒæ¨¡å‹æˆä¸º"è£åˆ¤"
- ç»™å®šå¤šä¸ªé€‰é¡¹ï¼Œé€‰å‡ºæœ€å®‰å…¨çš„
- é€‚åˆï¼šå·²æœ‰ä¿®å¤æ–¹æ¡ˆï¼Œéœ€è¦è¯„ä¼°

**æ–¹æ¡ˆäºŒï¼ˆä»£ç æ¸…æ´ï¼‰**ï¼š
- è®­ç»ƒæ¨¡å‹æˆä¸º"åŒ»ç”Ÿ"
- è¯Šæ–­å¹¶æ²»ç–—ä»£ç çš„"ç–¾ç—…"
- é€‚åˆï¼šè‡ªåŠ¨ä¿®å¤ï¼Œä¸»åŠ¨é˜²å¾¡

æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œæ–¹æ¡ˆäºŒæ›´åˆé€‚ï¼

