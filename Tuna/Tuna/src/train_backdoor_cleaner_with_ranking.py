# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆåé—¨æ¸…æ´å™¨ï¼šç»“åˆç”Ÿæˆå’Œæ’åºæŸå¤±

è®­ç»ƒç­–ç•¥ï¼š
    1. ä¸»è¦ç›®æ ‡ï¼šç”Ÿæˆè¯„åˆ†æœ€é«˜çš„å¹²å‡€ä»£ç ï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰
    2. è¾…åŠ©ç›®æ ‡ï¼šå­¦ä¹ æ‰€æœ‰å€™é€‰çš„ç›¸å¯¹è´¨é‡ï¼ˆæ’åºæŸå¤±ï¼‰
    3. æŸå¤±ç»„åˆï¼šalpha * ç”ŸæˆæŸå¤± + beta * æ’åºæŸå¤±
    
ä¼˜åŠ¿ï¼š
    - å……åˆ†åˆ©ç”¨æ•°æ®é›†ä¸­çš„æ‰€æœ‰å€™é€‰å’Œè¯„åˆ†ä¿¡æ¯
    - å­¦ä¹ å€™é€‰ä¹‹é—´çš„ç›¸å¯¹è´¨é‡å·®å¼‚
    - æå‡æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›
"""
import os
import json
import argparse
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    PreTrainedTokenizerBase,
)
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training


class RankingEnhancedDataset(Dataset):
    """
    å¢å¼ºæ•°æ®é›†ï¼šåŒæ—¶æ”¯æŒç”Ÿæˆå’Œæ’åº
    """
    def __init__(
        self,
        data_path: str,
        tokenizer: PreTrainedTokenizerBase,
        max_length: int = 2048,
        score_threshold: float = 50.0,
        use_all_candidates: bool = True,
    ):
        """
        Args:
            data_path: JSONLæ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            score_threshold: å¹²å‡€ä»£ç çš„æœ€ä½åˆ†æ•°é˜ˆå€¼
            use_all_candidates: æ˜¯å¦ä½¿ç”¨æ‰€æœ‰å€™é€‰ï¼ˆç”¨äºæ’åºå­¦ä¹ ï¼‰
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.score_threshold = score_threshold
        self.use_all_candidates = use_all_candidates
        
        print(f"ğŸš€ Loading ranking-enhanced data from {data_path}...")
        self.data = self._load_and_process_data(data_path)
        print(f"âœ… Loaded {len(self.data)} samples with ranking information")
        self._analyze_dataset()
        
    def _load_and_process_data(self, data_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®ï¼Œä¿ç•™æ‰€æœ‰å€™é€‰å’Œæ’åºä¿¡æ¯"""
        processed_data = []
        skipped = 0
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                    
                try:
                    sample = json.loads(line)
                    
                    input_code = sample.get('input', '')
                    outputs = sample.get('output', [])
                    scores = sample.get('score', [])
                    
                    # æ•°æ®éªŒè¯
                    if not input_code or not input_code.strip():
                        skipped += 1
                        continue
                    
                    if not outputs or not scores:
                        skipped += 1
                        continue
                    
                    if len(outputs) != len(scores):
                        print(f"âš ï¸ Warning: Line {line_num} mismatched outputs/scores")
                        skipped += 1
                        continue
                    
                    # æ‰¾åˆ°æœ€ä½³å€™é€‰
                    max_score = max(scores)
                    max_idx = scores.index(max_score)
                    
                    # æ£€æŸ¥é•¿åº¦ï¼ˆç²—ç•¥ä¼°ç®—ï¼š4å­—ç¬¦â‰ˆ1 tokenï¼‰
                    max_candidate_len = max(len(c) for c in outputs)
                    estimated_tokens = (len(input_code) + max_candidate_len) // 4
                    if estimated_tokens > self.max_length * 1.2:  # è¶…è¿‡æœ€å¤§é•¿åº¦20%å°±è·³è¿‡
                        skipped += 1
                        continue
                    
                    # åªä¿ç•™è¯„åˆ†è¶…è¿‡é˜ˆå€¼çš„æ ·æœ¬
                    if max_score >= self.score_threshold:
                        processed_data.append({
                            'id': sample.get('id', f'sample_{line_num}'),
                            'instruction': sample.get('instruction', ''),
                            'backdoored_code': input_code,
                            'candidates': outputs,  # ä¿ç•™æ‰€æœ‰å€™é€‰
                            'scores': scores,  # ä¿ç•™æ‰€æœ‰è¯„åˆ†
                            'best_idx': max_idx,  # æœ€ä½³å€™é€‰çš„ç´¢å¼•
                        })
                    else:
                        skipped += 1
                        
                except json.JSONDecodeError:
                    print(f"âš ï¸ Warning: Failed to parse line {line_num}")
                    skipped += 1
                    continue
        
        if skipped > 0:
            print(f"âš ï¸ Skipped {skipped} samples")
            
        return processed_data
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.data:
            return
        
        total = len(self.data)
        avg_candidates = sum(len(s['candidates']) for s in self.data) / total
        avg_input_len = sum(len(s['backdoored_code']) for s in self.data) / total
        avg_best_len = sum(len(s['candidates'][s['best_idx']]) for s in self.data) / total
        
        all_scores = []
        for s in self.data:
            all_scores.extend(s['scores'])
        
        print(f"ğŸ“Š Dataset Statistics:")
        print(f"   - Total samples: {total}")
        print(f"   - Avg candidates per sample: {avg_candidates:.1f}")
        print(f"   - Avg input length: {avg_input_len:.0f} chars")
        print(f"   - Avg best candidate length: {avg_best_len:.0f} chars")
        print(f"   - Score range: {min(all_scores):.1f} ~ {max(all_scores):.1f}")
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        è¿”å›è®­ç»ƒæ ·æœ¬ï¼ŒåŒ…å«ï¼š
        1. ç”Ÿæˆä»»åŠ¡çš„æ•°æ®ï¼ˆinput_ids, labelsï¼‰
        2. æ’åºä»»åŠ¡çš„æ•°æ®ï¼ˆæ‰€æœ‰å€™é€‰å’Œè¯„åˆ†ï¼‰
        """
        sample = self.data[idx]
        
        # === 1. ç”Ÿæˆä»»åŠ¡æ•°æ® ===
        prompt = self._create_prompt(sample['backdoored_code'])
        best_candidate = sample['candidates'][sample['best_idx']]
        
        prompt_text = f"{prompt}\n\n### Clean Code:\n"
        prompt_ids = self.tokenizer.encode(prompt_text, add_special_tokens=True)
        target_ids = self.tokenizer.encode(best_candidate, add_special_tokens=False)
        
        # æ‹¼æ¥å’Œå¤„ç†
        full_ids = prompt_ids + target_ids
        labels = [-100] * len(prompt_ids) + target_ids
        
        # æˆªæ–­å¤„ç†
        if len(full_ids) > self.max_length:
            max_target_len = self.max_length - len(prompt_ids)
            if max_target_len > 10:
                full_ids = prompt_ids + target_ids[:max_target_len]
                labels = [-100] * len(prompt_ids) + target_ids[:max_target_len]
            else:
                truncate_start = len(prompt_ids) - (self.max_length - 20)
                full_ids = prompt_ids[truncate_start:] + target_ids[:20]
                labels = [-100] * (self.max_length - 20) + target_ids[:20]
        
        # Padding
        padding_length = self.max_length - len(full_ids)
        if padding_length > 0:
            full_ids = full_ids + [self.tokenizer.pad_token_id] * padding_length
            labels = labels + [-100] * padding_length
        
        # === 2. æ’åºä»»åŠ¡æ•°æ® ===
        # ä¸ºæ‰€æœ‰å€™é€‰ç¼–ç ï¼ˆç”¨äºæ’åºå­¦ä¹ ï¼‰
        candidate_encodings = []
        for candidate in sample['candidates']:
            cand_text = f"{prompt_text}{candidate}"
            cand_encoding = self.tokenizer.encode(
                cand_text,
                max_length=self.max_length,
                truncation=True,
                add_special_tokens=True
            )
            # Padding
            if len(cand_encoding) < self.max_length:
                cand_encoding = cand_encoding + [self.tokenizer.pad_token_id] * (self.max_length - len(cand_encoding))
            candidate_encodings.append(cand_encoding)
        
        return {
            # ç”Ÿæˆä»»åŠ¡
            'input_ids': torch.tensor(full_ids, dtype=torch.long),
            'attention_mask': torch.tensor(
                [1 if tid != self.tokenizer.pad_token_id else 0 for tid in full_ids],
                dtype=torch.long
            ),
            'labels': torch.tensor(labels, dtype=torch.long),
            
            # æ’åºä»»åŠ¡
            'candidate_input_ids': torch.tensor(candidate_encodings, dtype=torch.long),
            'candidate_scores': torch.tensor(sample['scores'], dtype=torch.float),
        }
    
    def _create_prompt(self, backdoored_code: str) -> str:
        """åˆ›å»ºç®€æ´çš„æç¤ºè¯"""
        return f"""### Task: Remove backdoor triggers from the code

### Input Code:
{backdoored_code}
"""


class RankingEnhancedTrainer(Trainer):
    """
    å¢å¼ºTrainerï¼šåŒæ—¶ä¼˜åŒ–ç”Ÿæˆå’Œæ’åº
    """
    def __init__(self, *args, 
                 generation_weight: float = 1.0,
                 ranking_weight: float = 0.3,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.generation_weight = generation_weight
        self.ranking_weight = ranking_weight
        
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        è®¡ç®—ç»„åˆæŸå¤±ï¼šç”ŸæˆæŸå¤± + æ’åºæŸå¤±
        """
        # === 1. ç”ŸæˆæŸå¤±ï¼ˆä¸»è¦ç›®æ ‡ï¼‰===
        gen_inputs = {
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
            'labels': inputs['labels'],
        }
        outputs = model(**gen_inputs)
        generation_loss = outputs.loss
        
        # === 2. æ’åºæŸå¤±ï¼ˆè¾…åŠ©ç›®æ ‡ï¼‰===
        if 'candidate_input_ids' in inputs and self.ranking_weight > 0:
            ranking_loss = self._compute_ranking_loss(model, inputs)
        else:
            ranking_loss = torch.tensor(0.0, device=generation_loss.device)
        
        # === 3. ç»„åˆæŸå¤± ===
        total_loss = (
            self.generation_weight * generation_loss +
            self.ranking_weight * ranking_loss
        )
        
        # æ—¥å¿—
        if self.state.is_local_process_zero and self.is_in_train:
            self.log({
                'train_gen_loss': generation_loss.item(),
                'train_rank_loss': ranking_loss.item(),
                'train_total_loss': total_loss.item(),
            })
        
        return (total_loss, outputs) if return_outputs else total_loss
    
    def _compute_ranking_loss(self, model, inputs):
        """
        è®¡ç®—æ’åºæŸå¤±ï¼ˆListwise Ranking Lossï¼‰
        
        æ€è·¯ï¼š
        1. å¯¹æ¯ä¸ªå€™é€‰è®¡ç®—å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ä½œä¸ºè´¨é‡åˆ†æ•°
        2. ä½¿ç”¨ListMLEç®—æ³•å­¦ä¹ æ­£ç¡®çš„æ’åº
        """
        candidate_input_ids = inputs['candidate_input_ids']  # [batch, num_candidates, seq_len]
        true_scores = inputs['candidate_scores']  # [batch, num_candidates]
        
        batch_size, num_candidates, seq_len = candidate_input_ids.shape
        
        # å°†æ‰€æœ‰å€™é€‰å±•å¹³ï¼Œæ‰¹é‡è®¡ç®—
        flat_input_ids = candidate_input_ids.view(-1, seq_len)  # [batch*num_candidates, seq_len]
        flat_attention_mask = (flat_input_ids != self.tokenizer.pad_token_id).long()
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰çš„å›°æƒ‘åº¦ï¼ˆä½œä¸ºè´¨é‡çš„è´ŸæŒ‡æ ‡ï¼‰
        candidate_outputs = model(
            input_ids=flat_input_ids,
            attention_mask=flat_attention_mask,
        )
        # ä½¿ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶ä½œä¸ºè´¨é‡åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ â†’ è´Ÿå·åè¶Šé«˜è¶Šå¥½ï¼‰
        logits = candidate_outputs.logits  # [batch*num_candidates, seq_len, vocab_size]
        
        # è®¡ç®—å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶
        shift_logits = logits[..., :-1, :].contiguous()
        shift_input_ids = flat_input_ids[..., 1:].contiguous()
        
        # è®¡ç®—æ¯ä¸ªtokençš„è´Ÿå¯¹æ•°ä¼¼ç„¶
        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=self.tokenizer.pad_token_id)
        token_losses = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_input_ids.view(-1)
        )
        token_losses = token_losses.view(batch_size * num_candidates, -1)
        
        # å¹³å‡æ¯ä¸ªå€™é€‰çš„æŸå¤±ï¼ˆå›°æƒ‘åº¦çš„proxyï¼‰
        sequence_losses = token_losses.mean(dim=1)  # [batch*num_candidates]
        predicted_scores = -sequence_losses.view(batch_size, num_candidates)  # è´Ÿå·ï¼šlossè¶Šä½ï¼Œscoreè¶Šé«˜
        
        # === ListMLE æ’åºæŸå¤± ===
        ranking_loss_sum = []
        for i in range(batch_size):
            sample_pred = predicted_scores[i]
            sample_true = true_scores[i]
            
            # æŒ‰çœŸå®è¯„åˆ†æ’åº
            true_order = torch.argsort(sample_true, descending=True)
            pred_sorted_by_true = sample_pred[true_order]
            
            # ListMLE: æœ€å¤§åŒ–æ­£ç¡®æ’åºçš„æ¦‚ç‡
            sample_loss_terms = []
            for j in range(len(pred_sorted_by_true)):
                sample_loss_terms.append(-F.log_softmax(pred_sorted_by_true[j:], dim=0)[0])
            ranking_loss_sum.append(torch.stack(sample_loss_terms).sum())
        
        ranking_loss = torch.stack(ranking_loss_sum).mean() / num_candidates
        
        return ranking_loss


def str_to_bool(v):
    if isinstance(v, bool):
        return v
    if str(v).lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif str(v).lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def parse_arguments():
    parser = argparse.ArgumentParser(description="Train Backdoor Cleaner with Ranking Loss")
    
    # æ ¸å¿ƒè·¯å¾„
    group = parser.add_argument_group('Core Paths')
    group.add_argument("--model_name_or_path", type=str, required=True)
    group.add_argument("--train_data_path", type=str, required=True)
    group.add_argument("--eval_data_path", type=str, default=None)
    group.add_argument("--output_dir", type=str, required=True)
    
    # æ¨¡å‹é…ç½®
    group = parser.add_argument_group('Model Configuration')
    group.add_argument("--model_max_length", type=int, default=2048)
    group.add_argument("--use_lora", type=str_to_bool, default=True)
    group.add_argument("--lora_r", type=int, default=8)
    group.add_argument("--lora_alpha", type=int, default=16)
    group.add_argument("--lora_dropout", type=float, default=0.1)
    group.add_argument("--load_in_8bit", type=str_to_bool, default=False)
    
    # è®­ç»ƒè¶…å‚æ•°
    group = parser.add_argument_group('Training Hyperparameters')
    group.add_argument("--num_train_epochs", type=int, default=3)
    group.add_argument("--per_device_train_batch_size", type=int, default=2)  # å‡å°ï¼Œå› ä¸ºè¦å¤„ç†å¤šä¸ªå€™é€‰
    group.add_argument("--per_device_eval_batch_size", type=int, default=2)
    group.add_argument("--gradient_accumulation_steps", type=int, default=8)
    group.add_argument("--learning_rate", type=float, default=2e-5)
    group.add_argument("--warmup_steps", type=int, default=200)
    group.add_argument("--weight_decay", type=float, default=0.01)
    
    # æŸå¤±æƒé‡
    group = parser.add_argument_group('Loss Weights')
    group.add_argument("--generation_weight", type=float, default=1.0,
                      help="Weight for generation loss (main objective)")
    group.add_argument("--ranking_weight", type=float, default=0.3,
                      help="Weight for ranking loss (auxiliary objective)")
    
    # ç³»ç»Ÿé…ç½®
    group = parser.add_argument_group('System Configuration')
    group.add_argument("--fp16", type=str_to_bool, default=True)
    group.add_argument("--gradient_checkpointing", type=str_to_bool, default=True)
    group.add_argument("--logging_steps", type=int, default=10)
    group.add_argument("--save_steps", type=int, default=500)
    group.add_argument("--eval_steps", type=int, default=500)
    group.add_argument("--save_total_limit", type=int, default=3)
    
    return parser.parse_args()


def main():
    args = parse_arguments()
    
    print("=" * 70)
    print("ğŸ§¹ Backdoor Code Cleaner with Ranking Loss")
    print("=" * 70)
    print(f"ğŸ“ Model: {args.model_name_or_path}")
    print(f"ğŸ“ Train data: {args.train_data_path}")
    print(f"ğŸ“ Output: {args.output_dir}")
    print(f"âš–ï¸  Generation weight: {args.generation_weight}")
    print(f"âš–ï¸  Ranking weight: {args.ranking_weight}")
    print("=" * 70)
    
    # åŠ è½½tokenizer
    print("\nğŸ”§ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path,
        trust_remote_code=True,
        padding_side='right'
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ Loading model...")
    load_kwargs = {
        "trust_remote_code": True,
        "device_map": "auto",
    }
    
    if args.load_in_8bit:
        print("   Using 8-bit quantization...")
        load_kwargs["load_in_8bit"] = True
    else:
        load_kwargs["torch_dtype"] = torch.float16 if args.fp16 else torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **load_kwargs)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåœ¨åº”ç”¨ LoRA ä¹‹å‰ï¼‰
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        print("âœ… Gradient checkpointing enabled")
    
    # åº”ç”¨LoRA
    if args.use_lora:
        print(f"ğŸ”§ Applying LoRA (r={args.lora_r}, alpha={args.lora_alpha})...")
        
        # é‡åŒ–æ¨¡å‹éœ€è¦ç‰¹æ®Šå‡†å¤‡
        if args.load_in_8bit:
            model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)
            print("   Model prepared for quantized training")
        else:
            # éé‡åŒ–æ¨¡å‹ï¼Œæ‰‹åŠ¨å¯ç”¨è¾“å…¥åµŒå…¥çš„æ¢¯åº¦ï¼ˆLoRA éœ€è¦ï¼‰
            if hasattr(model, 'enable_input_require_grads'):
                model.enable_input_require_grads()
            else:
                def make_inputs_require_grad(module, input, output):
                    output.requires_grad_(True)
                model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            bias="none"
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“š Loading datasets...")
    train_dataset = RankingEnhancedDataset(
        args.train_data_path,
        tokenizer,
        args.model_max_length,
        use_all_candidates=True,
    )
    
    eval_dataset = None
    if args.eval_data_path and os.path.exists(args.eval_data_path):
        eval_dataset = RankingEnhancedDataset(
            args.eval_data_path,
            tokenizer,
            args.model_max_length,
            use_all_candidates=True,
        )
        print(f"âœ… Loaded {len(eval_dataset)} validation samples")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        weight_decay=args.weight_decay,
        
        fp16=args.fp16,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_steps=args.eval_steps if eval_dataset else None,
        save_total_limit=args.save_total_limit,
        
        eval_strategy="steps" if eval_dataset else "no",
        save_strategy="steps",
        load_best_model_at_end=True if eval_dataset else False,
        
        report_to=["tensorboard"],
        logging_dir=os.path.join(args.output_dir, "logs"),
        
        remove_unused_columns=False,
        ddp_find_unused_parameters=False,
    )
    
    # åˆ›å»ºTrainer
    trainer = RankingEnhancedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        generation_weight=args.generation_weight,
        ranking_weight=args.ranking_weight,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ Starting training with generation + ranking loss...")
    print("ğŸ“Œ Main objective: Generate clean code (generation loss)")
    print("ğŸ“Œ Auxiliary objective: Learn candidate quality (ranking loss)")
    print("=" * 70)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\nâœ… Training completed! Saving final model...")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    if eval_dataset:
        print("\nğŸ“Š Final evaluation...")
        eval_results = trainer.evaluate()
        print(f"Evaluation results: {eval_results}")
        
        with open(os.path.join(args.output_dir, "eval_results.json"), "w") as f:
            json.dump(eval_results, f, indent=2)
    
    print(f"\nğŸ‰ All done! Model saved to {args.output_dir}")
    print("=" * 70)


if __name__ == "__main__":
    main()

