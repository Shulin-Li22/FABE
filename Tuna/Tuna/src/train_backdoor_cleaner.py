# -*- coding: utf-8 -*-
"""
Backdoor Cleaner Training Script - åé—¨ä»£ç æ¸…æ´å™¨è®­ç»ƒè„šæœ¬

è®­ç»ƒç›®æ ‡ï¼š
    è®­ç»ƒæ¨¡å‹å­¦ä¼šä»åŒ…å«åé—¨çš„ä»£ç ç”Ÿæˆå¹²å‡€çš„ä»£ç ï¼Œæ¶ˆé™¤åé—¨è§¦å‘å™¨ã€‚
    è¿™æ˜¯ä¸€ä¸ªä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œè€Œä¸æ˜¯æ’åº/è¯„åˆ†ä»»åŠ¡ã€‚

è®­ç»ƒç­–ç•¥ï¼š
    1. è¾“å…¥ï¼šåŒ…å«åé—¨çš„åŸå§‹ä»£ç ï¼ˆinputå­—æ®µï¼‰
    2. ç›®æ ‡ï¼šè¯„åˆ†æœ€é«˜çš„å¹²å‡€ä»£ç å˜ä½“ï¼ˆoutputä¸­scoreæœ€é«˜çš„ï¼‰
    3. æŸå¤±ï¼šæ ‡å‡†çš„è¯­è¨€æ¨¡å‹äº¤å‰ç†µæŸå¤±
    4. é¢å¤–å¥–åŠ±ï¼šå¯¹æˆåŠŸæ¶ˆé™¤åé—¨æ¨¡å¼çš„é¢„æµ‹ç»™äºˆé¢å¤–å¥–åŠ±
"""
import os
import json
import argparse
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    PreTrainedTokenizerBase,
    DataCollatorForLanguageModeling
)
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training


class BackdoorCleaningDataset(Dataset):
    """
    åé—¨æ¸…æ´æ•°æ®é›†ï¼šè®­ç»ƒæ¨¡å‹ä»åŒ…å«åé—¨çš„ä»£ç ç”Ÿæˆå¹²å‡€ä»£ç 
    """
    def __init__(
        self,
        data_path: str,
        tokenizer: PreTrainedTokenizerBase,
        max_length: int = 2048,
        score_threshold: float = 50.0
    ):
        """
        Args:
            data_path: JSONLæ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            score_threshold: è®¤ä¸ºæ˜¯"å¹²å‡€ä»£ç "çš„æœ€ä½åˆ†æ•°é˜ˆå€¼
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.score_threshold = score_threshold
        
        print(f"ğŸš€ Loading backdoor cleaning data from {data_path}...")
        self.data = self._load_and_process_data(data_path)
        print(f"âœ… Loaded {len(self.data)} training pairs (inputâ†’clean output)")
        self._analyze_dataset()
        
    def _load_and_process_data(self, data_path: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ•°æ®å¹¶æå–è®­ç»ƒå¯¹ï¼š
        - è¾“å…¥ï¼šåŒ…å«åé—¨çš„åŸå§‹ä»£ç 
        - è¾“å‡ºï¼šè¯„åˆ†æœ€é«˜çš„å¹²å‡€ä»£ç 
        """
        processed_data = []
        skipped = 0
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                    
                try:
                    sample = json.loads(line)
                    
                    # è·å–è¾“å…¥ä»£ç å’Œæ‰€æœ‰è¾“å‡ºå˜ä½“
                    input_code = sample.get('input', '')
                    outputs = sample.get('output', [])
                    scores = sample.get('score', [])
                    
                    # æ•°æ®éªŒè¯
                    if not input_code or not input_code.strip():
                        skipped += 1
                        continue
                    
                    if not outputs or not scores:
                        skipped += 1
                        continue
                    
                    if len(outputs) != len(scores):
                        print(f"âš ï¸ Warning: Line {line_num} has mismatched outputs ({len(outputs)}) and scores ({len(scores)})")
                        skipped += 1
                        continue
                    
                    # æ‰¾åˆ°è¯„åˆ†æœ€é«˜çš„å˜ä½“ä½œä¸º"å¹²å‡€ä»£ç "ç›®æ ‡
                    max_score = max(scores)
                    max_idx = scores.index(max_score)
                    clean_code = outputs[max_idx]
                    
                    # æ£€æŸ¥é•¿åº¦ï¼ˆç²—ç•¥ä¼°ç®—ï¼š4å­—ç¬¦â‰ˆ1 tokenï¼‰
                    estimated_tokens = (len(input_code) + len(clean_code)) // 4
                    if estimated_tokens > self.max_length * 1.2:  # è¶…è¿‡æœ€å¤§é•¿åº¦20%å°±è·³è¿‡
                        skipped += 1
                        continue
                    
                    # åªæœ‰å½“æœ€é«˜åˆ†è¶…è¿‡é˜ˆå€¼æ—¶æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„å¹²å‡€ä»£ç 
                    if max_score >= self.score_threshold:
                        processed_data.append({
                            'id': sample.get('id', f'sample_{line_num}'),
                            'instruction': sample.get('instruction', ''),
                            'backdoored_code': input_code,
                            'clean_code': clean_code,
                            'clean_score': max_score
                        })
                    else:
                        skipped += 1
                        
                except json.JSONDecodeError:
                    print(f"âš ï¸ Warning: Failed to parse line {line_num}")
                    skipped += 1
                    continue
        
        if skipped > 0:
            print(f"âš ï¸ Skipped {skipped} samples (no valid clean code or score too low)")
            
        return processed_data
    
    def _analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.data:
            return
        
        total = len(self.data)
        avg_input_len = sum(len(s['backdoored_code']) for s in self.data) / total
        avg_output_len = sum(len(s['clean_code']) for s in self.data) / total
        avg_score = sum(s['clean_score'] for s in self.data) / total
        min_score = min(s['clean_score'] for s in self.data)
        max_score = max(s['clean_score'] for s in self.data)
        
        print(f"ğŸ“Š Dataset Statistics:")
        print(f"   - Total samples: {total}")
        print(f"   - Avg input length: {avg_input_len:.0f} chars")
        print(f"   - Avg output length: {avg_output_len:.0f} chars")
        print(f"   - Avg clean score: {avg_score:.1f}")
        print(f"   - Score range: {min_score:.1f} ~ {max_score:.1f}")
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        è¿”å›ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼š
        æ ¼å¼åŒ–ä¸º instruction-following çš„ç”Ÿæˆä»»åŠ¡
        """
        sample = self.data[idx]
        
        # æ„å»ºè¾“å…¥æç¤º
        prompt = self._create_cleaning_prompt(
            sample['instruction'],
            sample['backdoored_code']
        )
        
        # ç›®æ ‡è¾“å‡ºï¼ˆå¹²å‡€ä»£ç ï¼‰
        target = sample['clean_code']
        
        # ğŸ”§ ä¿®å¤ï¼šåˆ†åˆ« tokenize ç„¶åæ‹¼æ¥ï¼Œç¡®ä¿å¯¹åº”å…³ç³»æ­£ç¡®
        prompt_text = f"{prompt}\n\n### Clean Code:\n"
        
        # Tokenize promptï¼ˆåŒ…å« special tokensï¼‰
        prompt_ids = self.tokenizer.encode(prompt_text, add_special_tokens=True)
        
        # Tokenize targetï¼ˆä¸åŒ…å« special tokensï¼Œå› ä¸ºå·²ç»åœ¨ prompt ä¸­æ·»åŠ äº†ï¼‰
        target_ids = self.tokenizer.encode(target, add_special_tokens=False)
        
        # æ‹¼æ¥
        full_ids = prompt_ids + target_ids
        
        # åˆ›å»º labelsï¼šprompt éƒ¨åˆ†ç”¨ -100ï¼Œtarget éƒ¨åˆ†ç”¨å®é™… token id
        labels = [-100] * len(prompt_ids) + target_ids
        
        # å¤„ç†è¶…é•¿æƒ…å†µï¼šä¼˜å…ˆä¿ç•™ promptï¼Œæˆªæ–­ target
        if len(full_ids) > self.max_length:
            max_target_len = self.max_length - len(prompt_ids)
            if max_target_len > 10:  # è‡³å°‘ä¿ç•™ 10 ä¸ª token ç»™ target
                full_ids = prompt_ids + target_ids[:max_target_len]
                labels = [-100] * len(prompt_ids) + target_ids[:max_target_len]
            else:
                # prompt å¤ªé•¿ï¼Œå¿…é¡»æˆªæ–­ promptï¼ˆä¿ç•™ååŠéƒ¨åˆ†ï¼‰
                # è¿™ç§æƒ…å†µå¾ˆå°‘è§ï¼Œä½†è¦å¤„ç†
                truncate_start = len(prompt_ids) - (self.max_length - 20)  # ç»™ target ç•™ 20 tokens
                full_ids = prompt_ids[truncate_start:] + target_ids[:20]
                labels = [-100] * (self.max_length - 20) + target_ids[:20]
        
        # Padding åˆ° max_length
        padding_length = self.max_length - len(full_ids)
        if padding_length > 0:
            full_ids = full_ids + [self.tokenizer.pad_token_id] * padding_length
            labels = labels + [-100] * padding_length
        
        return {
            'input_ids': torch.tensor(full_ids, dtype=torch.long),
            'attention_mask': torch.tensor([1 if token_id != self.tokenizer.pad_token_id else 0 
                                           for token_id in full_ids], dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long)
        }
    
    def _create_cleaning_prompt(self, instruction: str, backdoored_code: str) -> str:
        """
        åˆ›å»ºä»£ç æ¸…æ´ä»»åŠ¡çš„æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ›´ç®€æ´ï¼ŒèŠ‚çœ80% tokensï¼‰
        """
        return f"""### Task: Remove backdoor triggers from the code

### Input Code:
{backdoored_code}
"""


class BackdoorCleaningTrainer(Trainer):
    """
    å¢å¼ºçš„Trainerï¼Œæ·»åŠ åé—¨æ£€æµ‹ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡
    """
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        è®¡ç®—æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±
        """
        outputs = model(**inputs)
        loss = outputs.loss
        
        return (loss, outputs) if return_outputs else loss


def str_to_bool(v: Any) -> bool:
    """å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼"""
    if isinstance(v, bool):
        return v
    if str(v).lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif str(v).lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Train a Backdoor Code Cleaner")
    
    # æ ¸å¿ƒè·¯å¾„
    group = parser.add_argument_group('Core Paths')
    group.add_argument("--model_name_or_path", type=str, required=True, 
                      help="Path to the pretrained base model.")
    group.add_argument("--train_data_path", type=str, required=True, 
                      help="Path to the training data JSONL file.")
    group.add_argument("--eval_data_path", type=str, default=None, 
                      help="Path to the validation data JSONL file.")
    group.add_argument("--output_dir", type=str, required=True, 
                      help="Directory to save checkpoints and final model.")
    
    # æ¨¡å‹é…ç½®
    group = parser.add_argument_group('Model Configuration')
    group.add_argument("--model_max_length", type=int, default=2048, 
                      help="Maximum sequence length.")
    group.add_argument("--use_lora", type=str_to_bool, default=True, 
                      help="Use LoRA for efficient fine-tuning.")
    group.add_argument("--lora_r", type=int, default=8, 
                      help="LoRA rank.")
    group.add_argument("--lora_alpha", type=int, default=16, 
                      help="LoRA alpha parameter.")
    group.add_argument("--lora_dropout", type=float, default=0.1, 
                      help="LoRA dropout rate.")
    group.add_argument("--load_in_8bit", type=str_to_bool, default=False,
                      help="Load model in 8-bit quantization (saves memory).")
    group.add_argument("--load_in_4bit", type=str_to_bool, default=False,
                      help="Load model in 4-bit quantization (saves more memory).")
    
    # è®­ç»ƒè¶…å‚æ•°
    group = parser.add_argument_group('Training Hyperparameters')
    group.add_argument("--num_train_epochs", type=int, default=3, 
                      help="Number of training epochs.")
    group.add_argument("--per_device_train_batch_size", type=int, default=4, 
                      help="Batch size per device during training.")
    group.add_argument("--per_device_eval_batch_size", type=int, default=4, 
                      help="Batch size per device during evaluation.")
    group.add_argument("--gradient_accumulation_steps", type=int, default=4, 
                      help="Gradient accumulation steps.")
    group.add_argument("--learning_rate", type=float, default=2e-5, 
                      help="Initial learning rate.")
    group.add_argument("--warmup_steps", type=int, default=200, 
                      help="Warmup steps.")
    group.add_argument("--weight_decay", type=float, default=0.01, 
                      help="Weight decay.")
    
    # ç³»ç»Ÿé…ç½®
    group = parser.add_argument_group('System Configuration')
    group.add_argument("--fp16", type=str_to_bool, default=True, 
                      help="Use FP16 mixed precision training.")
    group.add_argument("--gradient_checkpointing", type=str_to_bool, default=True, 
                      help="Enable gradient checkpointing.")
    group.add_argument("--logging_steps", type=int, default=10, 
                      help="Log every N steps.")
    group.add_argument("--save_steps", type=int, default=500, 
                      help="Save checkpoint every N steps.")
    group.add_argument("--eval_steps", type=int, default=500, 
                      help="Evaluate every N steps.")
    group.add_argument("--save_total_limit", type=int, default=3, 
                      help="Maximum number of checkpoints to keep.")
    
    return parser.parse_args()


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    args = parse_arguments()
    
    print("=" * 60)
    print("ğŸ§¹ Backdoor Code Cleaner Training")
    print("=" * 60)
    print(f"ğŸ“ Model: {args.model_name_or_path}")
    print(f"ğŸ“ Train data: {args.train_data_path}")
    print(f"ğŸ“ Output: {args.output_dir}")
    print("=" * 60)
    
    # åŠ è½½tokenizer
    print("\nğŸ”§ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path,
        trust_remote_code=True,
        padding_side='right'  # é‡è¦ï¼šç”Ÿæˆä»»åŠ¡ä½¿ç”¨right padding
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ Loading model...")
    load_kwargs = {
        "trust_remote_code": True,
        "device_map": "auto",
    }
    
    # é‡åŒ–é…ç½®
    if args.load_in_8bit:
        print("   Using 8-bit quantization...")
        load_kwargs["load_in_8bit"] = True
    elif args.load_in_4bit:
        print("   Using 4-bit quantization...")
        load_kwargs["load_in_4bit"] = True
    else:
        load_kwargs["torch_dtype"] = torch.float16 if args.fp16 else torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **load_kwargs)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåœ¨åº”ç”¨ LoRA ä¹‹å‰ï¼‰
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        print("âœ… Gradient checkpointing enabled")
    
    # åº”ç”¨LoRA
    if args.use_lora:
        print(f"ğŸ”§ Applying LoRA (r={args.lora_r}, alpha={args.lora_alpha})...")
        
        # é‡åŒ–æ¨¡å‹éœ€è¦ç‰¹æ®Šå‡†å¤‡
        if args.load_in_8bit or args.load_in_4bit:
            model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)
            print("   Model prepared for quantized training")
        else:
            # éé‡åŒ–æ¨¡å‹ï¼Œæ‰‹åŠ¨å¯ç”¨è¾“å…¥åµŒå…¥çš„æ¢¯åº¦ï¼ˆLoRA éœ€è¦ï¼‰
            if hasattr(model, 'enable_input_require_grads'):
                model.enable_input_require_grads()
            else:
                def make_inputs_require_grad(module, input, output):
                    output.requires_grad_(True)
                model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            bias="none"
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“š Loading datasets...")
    train_dataset = BackdoorCleaningDataset(
        args.train_data_path,
        tokenizer,
        args.model_max_length
    )
    
    eval_dataset = None
    if args.eval_data_path and os.path.exists(args.eval_data_path):
        eval_dataset = BackdoorCleaningDataset(
            args.eval_data_path,
            tokenizer,
            args.model_max_length
        )
        print(f"âœ… Loaded {len(eval_dataset)} validation samples")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        weight_decay=args.weight_decay,
        
        fp16=args.fp16,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_steps=args.eval_steps if eval_dataset else None,
        save_total_limit=args.save_total_limit,
        
        eval_strategy="steps" if eval_dataset else "no",
        save_strategy="steps",
        load_best_model_at_end=True if eval_dataset else False,
        
        report_to=["tensorboard"],
        logging_dir=os.path.join(args.output_dir, "logs"),
        
        remove_unused_columns=False,
        ddp_find_unused_parameters=False,
    )
    
    # åˆ›å»ºTrainer
    trainer = BackdoorCleaningTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ Starting backdoor cleaning training...")
    print("ğŸ“Œ Training objective: Learn to generate clean code from backdoored code")
    print("=" * 60)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\nâœ… Training completed! Saving final model...")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    if eval_dataset:
        print("\nğŸ“Š Final evaluation...")
        eval_results = trainer.evaluate()
        print(f"Evaluation results: {eval_results}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        with open(os.path.join(args.output_dir, "eval_results.json"), "w") as f:
            json.dump(eval_results, f, indent=2)
    
    print(f"\nğŸ‰ All done! Model saved to {args.output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()

