# train_backdoor_cleaner.py ä»£ç æ”¹è¿›å»ºè®®

## ğŸ”´ ä¸¥é‡é—®é¢˜ï¼ˆéœ€è¦ä¿®å¤ï¼‰

### 1. Tokenization é€»è¾‘é”™è¯¯ âš ï¸

**å½“å‰ä»£ç ï¼ˆLine 137-149ï¼‰**ï¼š
```python
prompt_ids = self.tokenizer.encode(
    f"{prompt}\n\n### Clean Code:\n",
    add_special_tokens=True
)
full_ids = self.tokenizer.encode(
    full_text,
    max_length=self.max_length,
    truncation=True,
    add_special_tokens=True
)

# å‡è®¾ full_ids çš„å‰ len(prompt_ids) ä¸ª token å°±æ˜¯ prompt_ids
labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]
```

**é—®é¢˜**ï¼š
- **ä¸æ­£ç¡®çš„å‡è®¾**ï¼š`encode(A+B)` çš„å‰ N ä¸ª token ä¸ä¸€å®šç­‰äº `encode(A)`
- Tokenizer å¯èƒ½ä¼šå› ä¸ºä¸Šä¸‹æ–‡è€Œäº§ç”Ÿä¸åŒçš„ token åºåˆ—
- å¦‚æœ prompt_ids é•¿åº¦ > full_ids é•¿åº¦ï¼Œä¼šå¯¼è‡´é”™è¯¯

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ¡ˆ1: åˆ†åˆ« tokenize åæ‹¼æ¥
prompt_text = f"{prompt}\n\n### Clean Code:\n"
target_text = target

prompt_tokens = self.tokenizer.encode(prompt_text, add_special_tokens=True)
target_tokens = self.tokenizer.encode(target_text, add_special_tokens=False)  # ä¸åŠ  special tokens

# æ‹¼æ¥
full_ids = prompt_tokens + target_tokens
labels = [-100] * len(prompt_tokens) + target_tokens

# æˆªæ–­å’Œ padding
if len(full_ids) > self.max_length:
    # ä¼˜å…ˆä¿ç•™ promptï¼Œæˆªæ–­ target
    max_target_len = self.max_length - len(prompt_tokens)
    if max_target_len > 0:
        full_ids = prompt_tokens + target_tokens[:max_target_len]
        labels = [-100] * len(prompt_tokens) + target_tokens[:max_target_len]
    else:
        # prompt å¤ªé•¿ï¼Œå¿…é¡»æˆªæ–­ prompt
        full_ids = full_ids[:self.max_length]
        labels = labels[:self.max_length]

# Padding
padding_length = self.max_length - len(full_ids)
if padding_length > 0:
    full_ids = full_ids + [self.tokenizer.pad_token_id] * padding_length
    labels = labels + [-100] * padding_length
```

### 2. ç¼ºå°‘æ•°æ®éªŒè¯

**é—®é¢˜**ï¼š
- æ²¡æœ‰éªŒè¯ input å’Œ output æ˜¯å¦ä¸ºç©º
- æ²¡æœ‰æ£€æŸ¥ score å’Œ output æ•°é‡æ˜¯å¦åŒ¹é…

**ä¿®å¤**ï¼š
```python
# åœ¨ _load_and_process_data ä¸­æ·»åŠ 
if not input_code or not input_code.strip():
    skipped += 1
    continue

if len(outputs) != len(scores):
    print(f"âš ï¸ Warning: Sample {line_num} has mismatched outputs and scores")
    skipped += 1
    continue
```

---

## ğŸŸ¡ é‡è¦ä¼˜åŒ–ï¼ˆå»ºè®®å®ç°ï¼‰

### 3. ä¼˜åŒ–æç¤ºè¯é•¿åº¦

**å½“å‰é—®é¢˜**ï¼š
- æç¤ºè¯å¾ˆé•¿ï¼ˆçº¦ 200 tokensï¼‰ï¼Œå ç”¨äº†å¤§é‡ context window
- å¯¹äº max_length=2048ï¼Œå®é™…ç•™ç»™ä»£ç çš„ç©ºé—´ä¸è¶³

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
def _create_cleaning_prompt(self, instruction: str, backdoored_code: str) -> str:
    # ç®€åŒ–ç‰ˆæç¤ºè¯
    return f"""### Task: Remove backdoor triggers from the code below

### Input Code:
{backdoored_code}

### Clean Code (backdoor-free):"""
```

**å¯¹æ¯”**ï¼š
- åŸç‰ˆï¼š~15 è¡Œï¼Œçº¦ 200+ tokens
- ä¼˜åŒ–ç‰ˆï¼š5 è¡Œï¼Œçº¦ 40 tokens
- **èŠ‚çœ 80% çš„ prompt ç©ºé—´**

### 4. æ·»åŠ æ•°æ®ç»Ÿè®¡å’Œåˆ†æ

**å»ºè®®æ·»åŠ **ï¼š
```python
def _analyze_dataset(self):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    total_samples = len(self.data)
    
    avg_input_len = sum(len(s['backdoored_code']) for s in self.data) / total_samples
    avg_output_len = sum(len(s['clean_code']) for s in self.data) / total_samples
    avg_score = sum(s['clean_score'] for s in self.data) / total_samples
    
    print(f"ğŸ“Š Dataset Statistics:")
    print(f"   - Total samples: {total_samples}")
    print(f"   - Avg input length: {avg_input_len:.0f} chars")
    print(f"   - Avg output length: {avg_output_len:.0f} chars")
    print(f"   - Avg clean score: {avg_score:.1f}")
    print(f"   - Score range: {min(s['clean_score'] for s in self.data):.1f} ~ {max(s['clean_score'] for s in self.data):.1f}")

# åœ¨ __init__ ä¸­è°ƒç”¨
self._analyze_dataset()
```

### 5. æ”¹è¿›æ¨¡å‹åŠ è½½é€»è¾‘

**å½“å‰é—®é¢˜**ï¼š
- `prepare_model_for_kbit_training` åœ¨ä¸ä½¿ç”¨é‡åŒ–æ—¶ä¸éœ€è¦
- ç¼ºå°‘å¯¹ 8-bit æˆ– 4-bit é‡åŒ–çš„æ”¯æŒé€‰é¡¹

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
# æ·»åŠ é‡åŒ–é€‰é¡¹
group.add_argument("--load_in_8bit", type=str_to_bool, default=False,
                  help="Load model in 8-bit quantization.")
group.add_argument("--load_in_4bit", type=str_to_bool, default=False,
                  help="Load model in 4-bit quantization.")

# åœ¨ main() ä¸­
model = AutoModelForCausalLM.from_pretrained(
    args.model_name_or_path,
    trust_remote_code=True,
    torch_dtype=torch.float16 if args.fp16 else torch.float32,
    load_in_8bit=args.load_in_8bit,
    load_in_4bit=args.load_in_4bit,
    device_map="auto"
)

# åªåœ¨ä½¿ç”¨é‡åŒ–æ—¶è°ƒç”¨
if args.load_in_8bit or args.load_in_4bit:
    model = prepare_model_for_kbit_training(model)
```

### 6. æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

**å½“å‰é—®é¢˜**ï¼š
- åªæœ‰æ ‡å‡†çš„ lossï¼Œæ— æ³•è¯„ä¼°ç”Ÿæˆè´¨é‡

**å»ºè®®æ·»åŠ **ï¼š
```python
def compute_metrics(self, eval_preds):
    """è®¡ç®—è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡"""
    predictions, labels = eval_preds
    
    # è§£ç é¢„æµ‹å’ŒçœŸå®æ–‡æœ¬
    decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # è®¡ç®—æŒ‡æ ‡
    exact_match = sum(p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels))
    exact_match_rate = exact_match / len(decoded_preds)
    
    # æ£€æµ‹åé—¨æ¨¡å¼æ˜¯å¦è¢«ç§»é™¤
    backdoor_patterns = ['for(int k=0;k<0;k++)', 'volatile char', 'while(0)']
    backdoor_removed = 0
    for pred in decoded_preds:
        if not any(pattern in pred for pattern in backdoor_patterns):
            backdoor_removed += 1
    backdoor_removal_rate = backdoor_removed / len(decoded_preds)
    
    return {
        'exact_match_rate': exact_match_rate,
        'backdoor_removal_rate': backdoor_removal_rate,
    }

# åœ¨åˆ›å»º Trainer æ—¶
trainer = BackdoorCleaningTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=self.compute_metrics,  # æ·»åŠ è¿™è¡Œ
)
```

---

## ğŸŸ¢ æ¬¡è¦ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### 7. æ·»åŠ æ—©åœæœºåˆ¶

```python
from transformers import EarlyStoppingCallback

# åœ¨ TrainingArguments ä¸­æ·»åŠ 
training_args = TrainingArguments(
    # ... å…¶ä»–å‚æ•°
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

# æ·»åŠ  callback
trainer = BackdoorCleaningTrainer(
    # ... å…¶ä»–å‚æ•°
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
```

### 8. æ·»åŠ è¿›åº¦æ¡å’Œæ—¥å¿—

```python
from tqdm import tqdm

def _load_and_process_data(self, data_path: str) -> List[Dict[str, Any]]:
    processed_data = []
    skipped = 0
    
    # å…ˆè¯»å–æ‰€æœ‰è¡Œä»¥è·å–æ€»æ•°
    with open(data_path, 'r', encoding='utf-8') as f:
        lines = [line for line in f if line.strip()]
    
    # ä½¿ç”¨è¿›åº¦æ¡
    for line_num, line in enumerate(tqdm(lines, desc="Loading data"), 1):
        # ... å¤„ç†é€»è¾‘
```

### 9. æ”¯æŒä» checkpoint æ¢å¤è®­ç»ƒ

```python
# åœ¨ parse_arguments ä¸­æ·»åŠ 
group.add_argument("--resume_from_checkpoint", type=str, default=None,
                  help="Path to checkpoint to resume training from.")

# åœ¨ trainer.train() ä¸­
trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
```

### 10. æ·»åŠ æµ‹è¯•é›†æ”¯æŒ

```python
# åœ¨ parse_arguments ä¸­æ·»åŠ 
group.add_argument("--test_data_path", type=str, default=None,
                  help="Path to the test data JSONL file.")

# åœ¨ main() ä¸­
test_dataset = None
if args.test_data_path and os.path.exists(args.test_data_path):
    test_dataset = BackdoorCleaningDataset(
        args.test_data_path,
        tokenizer,
        args.model_max_length
    )
    print(f"âœ… Loaded {len(test_dataset)} test samples")

# è®­ç»ƒåè¿›è¡Œæµ‹è¯•
if test_dataset:
    print("\nğŸ§ª Testing on test set...")
    test_results = trainer.evaluate(test_dataset, metric_key_prefix="test")
    print(f"Test results: {test_results}")
    
    with open(os.path.join(args.output_dir, "test_results.json"), "w") as f:
        json.dump(test_results, f, indent=2)
```

---

## ğŸ“‹ ä¼˜å…ˆçº§æ’åº

**å¿…é¡»ä¿®å¤**ï¼š
1. âœ… Tokenization é€»è¾‘ï¼ˆä¼šå¯¼è‡´è®­ç»ƒé”™è¯¯ï¼‰
2. âœ… æ•°æ®éªŒè¯ï¼ˆé˜²æ­¢å´©æºƒï¼‰

**å¼ºçƒˆå»ºè®®**ï¼š
3. ä¼˜åŒ–æç¤ºè¯é•¿åº¦ï¼ˆæå‡æ€§èƒ½ï¼‰
4. æ·»åŠ æ•°æ®ç»Ÿè®¡ï¼ˆäº†è§£æ•°æ®åˆ†å¸ƒï¼‰
5. æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼ˆç›‘æ§è®­ç»ƒæ•ˆæœï¼‰

**å¯é€‰ä¼˜åŒ–**ï¼š
6. é‡åŒ–æ”¯æŒï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
7. æ—©åœæœºåˆ¶ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
8. æµ‹è¯•é›†æ”¯æŒï¼ˆå®Œæ•´è¯„ä¼°æµç¨‹ï¼‰

---

## ğŸ¯ æœ€å…³é”®çš„æ”¹è¿›

å¦‚æœåªèƒ½é€‰ä¸€ä¸ªæ”¹è¿›ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä¿®å¤ **Tokenization é€»è¾‘**ï¼Œå› ä¸ºï¼š
- å½“å‰å®ç°å¯èƒ½å¯¼è‡´è®­ç»ƒæ•°æ®ä¸æ­£ç¡®
- labels å’Œ input_ids çš„å¯¹åº”å…³ç³»å¯èƒ½é”™ä½
- ä¼šä¸¥é‡å½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœ

ä¿®å¤åï¼Œæ¨¡å‹æ‰èƒ½æ­£ç¡®å­¦ä¹  "å“ªéƒ¨åˆ†æ˜¯æç¤ºè¯ï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰ï¼Œå“ªéƒ¨åˆ†æ˜¯ç›®æ ‡ä»£ç ï¼ˆè®¡ç®—æŸå¤±ï¼‰"ã€‚

