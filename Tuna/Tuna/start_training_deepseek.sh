#!/bin/bash

# DeepSeek-Coder 6.7B åé—¨æ¸…æ´å™¨è®­ç»ƒè„šæœ¬
# ä½¿ç”¨ç”Ÿæˆ+æ’åºæŸå¤±çš„å¢å¼ºè®­ç»ƒæ–¹æ³•

# æ¿€æ´» conda ç¯å¢ƒ
#source /home/nfs/u2023-ckh/miniconda3/bin/activate fabe

export CUDA_VISIBLE_DEVICES=0
export TOKENIZERS_PARALLELISM=false
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# è·¯å¾„é…ç½®
MODEL_PATH="/home/nfs/share-yjy/dachuang2025/models/deepseek-coder-6.7b-instruct"
DATA_DIR="/home/nfs/share-yjy/dachuang2025/m2026-lsl/FABE/Tuna/Tuna/data"
OUTPUT_DIR="/home/nfs/share-yjy/dachuang2025/m2026-lsl/FABE/Tuna/Tuna/checkpoints/backdoor_cleaner_deepseek_6.7b"

# æ•°æ®æ–‡ä»¶
TRAIN_FILE="${DATA_DIR}/train.jsonl"
VALID_FILE="${DATA_DIR}/valid.jsonl"
TEST_FILE="${DATA_DIR}/test.jsonl"

# è®­ç»ƒå‚æ•°
NUM_EPOCHS=3
BATCH_SIZE=4              # æé«˜åˆ°4ï¼Œåˆ©ç”¨A100 80GBçš„å……è£•æ˜¾å­˜
GRAD_ACCUM=4              # é™åˆ°4ï¼Œä¿æŒæœ‰æ•ˆbatch = 4*4 = 16
MAX_LENGTH=4096           # æ”¹ä¸º4096ä»¥è¦†ç›–æ›´å¤šæ ·æœ¬
LR=2e-5

# LoRA é…ç½®
LORA_R=8
LORA_ALPHA=16
LORA_DROPOUT=0.1

# æŸå¤±æƒé‡ï¼ˆFABEç»„åˆæŸå¤±å‡½æ•°ï¼‰
GENERATION_WEIGHT=1.0    # MLE Loss: ç”Ÿæˆå¹²å‡€ä»£ç 
RANKING_WEIGHT=0.3       # Listwise Ranking Loss: ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ${OUTPUT_DIR}

echo "=============================================="
echo "ğŸš€ å¼€å§‹è®­ç»ƒ DeepSeek-Coder 6.7B åé—¨æ¸…æ´å™¨"
echo "=============================================="
echo "ğŸ“ æ¨¡å‹: ${MODEL_PATH}"
echo "ğŸ“Š æ•°æ®é›†:"
echo "   - è®­ç»ƒé›†: ${TRAIN_FILE} (21,854 æ ·æœ¬)"
echo "   - éªŒè¯é›†: ${VALID_FILE} (2,732 æ ·æœ¬)"
echo "   - æµ‹è¯•é›†: ${TEST_FILE} (2,732 æ ·æœ¬)"
echo "ğŸ“ˆ è®­ç»ƒç­–ç•¥:"
echo "   - ç”ŸæˆæŸå¤±æƒé‡: ${GENERATION_WEIGHT}"
echo "   - æ’åºæŸå¤±æƒé‡: ${RANKING_WEIGHT}"
echo "   - LoRA: r=${LORA_R}, alpha=${LORA_ALPHA}"
echo "ğŸ’¾ è¾“å‡º: ${OUTPUT_DIR}"
echo "=============================================="

cd /home/nfs/share-yjy/dachuang2025/m2026-lsl/FABE/Tuna/Tuna/src

# å¯åŠ¨è®­ç»ƒ
nohup python train_backdoor_cleaner_with_ranking.py \
    --model_name_or_path ${MODEL_PATH} \
    --train_data_path ${TRAIN_FILE} \
    --eval_data_path ${VALID_FILE} \
    --output_dir ${OUTPUT_DIR} \
    --num_train_epochs ${NUM_EPOCHS} \
    --per_device_train_batch_size ${BATCH_SIZE} \
    --per_device_eval_batch_size ${BATCH_SIZE} \
    --gradient_accumulation_steps ${GRAD_ACCUM} \
    --model_max_length ${MAX_LENGTH} \
    --learning_rate ${LR} \
    --warmup_steps 200 \
    --weight_decay 0.01 \
    --use_lora True \
    --lora_r ${LORA_R} \
    --lora_alpha ${LORA_ALPHA} \
    --lora_dropout ${LORA_DROPOUT} \
    --load_in_8bit False \
    --generation_weight ${GENERATION_WEIGHT} \
    --ranking_weight ${RANKING_WEIGHT} \
    --fp16 True \
    --gradient_checkpointing True \
    --logging_steps 10 \
    --save_steps 500 \
    --eval_steps 500 \
    --save_total_limit 3 \
    > ${OUTPUT_DIR}/training.log 2>&1 &

TRAINING_PID=$!
echo $TRAINING_PID > /home/nfs/share-yjy/dachuang2025/m2026-lsl/FABE/Tuna/Tuna/training_deepseek.pid

echo ""
echo "âœ… è®­ç»ƒå·²å¯åŠ¨ï¼"
echo "ğŸ”¢ è¿›ç¨‹ ID: ${TRAINING_PID}"
echo ""
echo "ğŸ“Š ç›‘æ§å‘½ä»¤:"
echo "  # æŸ¥çœ‹å®æ—¶æ—¥å¿—"
echo "  tail -f ${OUTPUT_DIR}/training.log"
echo ""
echo "  # æŸ¥çœ‹ TensorBoard"
echo "  tensorboard --logdir ${OUTPUT_DIR}/logs"
echo ""
echo "  # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€"
echo "  ps aux | grep ${TRAINING_PID}"
echo ""
echo "  # æŸ¥çœ‹ GPU ä½¿ç”¨"
echo "  watch -n 1 nvidia-smi"
echo ""

